import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

%matplotlib inline
from scipy.stats import skew
plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = (12,8)

advert = pd.read_csv('Advertising.csv')
advert.head()
advert.info()

#plot the relation between different independent variables and final dependent variable
sns.pairplot(advert ,x_vars=['TV','newspaper','radio'] ,y_vars='sales' ,height=7 ,aspect=0.7)

from sklearn.linear_model import LinearRegression
X = advert[['TV','radio','newspaper']]
y = advert.sales

lm1 = LinearRegression()
lm1.fit(X, y)

print(lm1.intercept_)
print(lm1.coef_)

list(zip(['TV','radio','newspaper'],lm1.coef_))

#plot the correlation matrix
sns.heatmap(advert.corr(), annot=True)

'''
When we perform multiple linear regression we like to know 
1. Atleast one of our predictors useful in predicting the response.
2. Is all predictors are useful in predicting or is it only a subset of predictors are useful.
3. How well our model predict the data.
4. How accurate is our prediction.

It is more often the case that subset of predictors are useful for predicting the response.
This task of determining which predictors are associated with the response in order to fit a single model 
involving only those predictors this process is known as "feature selection".
'''

#calculating R2 scores for different predictors
from sklearn.metrics import r2_score

lm2 = LinearRegression().fit(X[['TV','radio']],y)
lm2_preds = lm2.predict(X[['TV','radio']])

print('R^2 scores: ', r2_score(y, lm2_preds))

lm3 = LinearRegression().fit(X[['TV','radio','newspaper']],y)
lm3_preds = lm3.predict(X[['TV','radio','newspaper']])

print('R^2 scores: ', r2_score(y, lm3_preds))

# R-square is equal to the square of the correlation between the response and the fitted linear model.

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

X = advert[['TV','newspaper','radio']]
y = advert.sales
X_train, X_test , y_train, y_test = train_test_split(X ,y ,random_state=1) 
#random_state is used to apply the same split for all model we are gonna create 

lm4 = LinearRegression().fit(X_train,y_train)
lm4_preds = lm4.predict(X_test)

print('RMSE error score: ',np.sqrt(mean_squared_error(y_test, lm4_preds)))

#Test the model with only 2 predictors
X = advert[['TV','radio']]
y = advert.sales
X_train, X_test , y_train, y_test = train_test_split(X ,y ,random_state=1) 

lm5 = LinearRegression().fit(X_train,y_train)
lm5_preds = lm5.predict(X_test)

print('RMSE error score: ',np.sqrt(mean_squared_error(y_test, lm5_preds)))
print('R^2 error: ',r2_score(y_pred, lm5_preds))

#plot our regression plot using amazing "yellowbrick" data visualization and model diagnostic library
from yellowbrick.regressor import PredictionError, ResidualsPlot

visualizer = PredictionError(lm5).fit(X_train, y_train)
visualizer.score(X_test, y_test)

visualizer.poof()

'''
An interaction effect is the simultaneous effect of two or more independent variables
on at least one dependent variable in which their joint effect is significantly greater 
(or significantly less) than the sum of the parts
'''

#we are gonna include an interaction term which is equal to X1 * X2 * X3

advert['interaction'] = advert['TV'] * advert['radio']

X = advert[['TV','radio','interaction']]
y = advert.sales
X_train, X_test , y_train, y_test = train_test_split(X ,y ,random_state=1) 

lm6 = LinearRegression().fit(X_train,y_train)
lm6_preds = lm6.predict(X_test)

print('RMSE error score: ',np.sqrt(mean_squared_error(y_test, lm6_preds)))
print('R^2 error: ',r2_score(y_pred, lm6_preds))

visualizer = PredictionError(lm6).fit(X_train, y_train)
visualizer.score(X_test, y_test)

visualizer.poof()

'''
the last model lm6 explains 97% variability in our data 
compared to lm5 model which explains 91% variability
'''
